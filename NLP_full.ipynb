{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2hkRLZsE8MNIksD0yKzQe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iameeshan26/Natural_language_processing/blob/main/NLP_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpav_AYhfqr2",
        "outputId": "4943055f-112f-4340-88a4-722b6aa24887"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "nltk.download('wordnet')\n",
        "from nltk import pos_tag\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_word(text):\n",
        "  token=word_tokenize(text)\n",
        "  return token"
      ],
      "metadata": {
        "id": "dnNJVNL0qkvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"NLP is a leading platform for building python programs to work with human language!\"\n",
        "print(tokenize_word(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG5w5HI1qyGm",
        "outputId": "34603cdd-6501-4ea7-b9de-edda2147d5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'is', 'a', 'leading', 'platform', 'for', 'building', 'python', 'programs', 'to', 'work', 'with', 'human', 'language', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "  words=word_tokenize(text)\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  word_tokens=word_tokenize(text)\n",
        "  filtered_words=[word for word in words if word.lower() not in stop_words]\n",
        "  filtered_text='_'.join(filtered_words)\n",
        "  return filtered_text"
      ],
      "metadata": {
        "id": "k4ckaqoAf2l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"NLP is a leading platform for building python programs to work with human language!\"\n",
        "print(remove_stopwords(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAGyhdzOjJkH",
        "outputId": "26168618-84cb-4a9f-fe88-e2cfa0cf074d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP_leading_platform_building_python_programs_work_human_language_!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sentence(text):\n",
        "  sent=sent_tokenize(text)\n",
        "  return sent"
      ],
      "metadata": {
        "id": "uN17OsCLjOFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"NLTK is leading platform for building python programs to work with human language data. It provides easy to use interfaces to over 50 corpora and lexical resources such as wordnet along with a suit of text processing libraries for classification, tokenization, stemming, tagging, parsing and semantic reasoning, wrappers for industrial strength NLP libraries, and active discussion forum.\"\n",
        "sentences=tokenize_sentence(text)\n",
        "for i,sentence in enumerate(sentences):\n",
        "  print(f\"Sentence {i+1}: {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw2Qy3c5opwq",
        "outputId": "e458c59a-59eb-4aa5-c204-c2a7d9b7bd20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: NLTK is leading platform for building python programs to work with human language data.\n",
            "Sentence 2: It provides easy to use interfaces to over 50 corpora and lexical resources such as wordnet along with a suit of text processing libraries for classification, tokenization, stemming, tagging, parsing and semantic reasoning, wrappers for industrial strength NLP libraries, and active discussion forum.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_text(text):\n",
        "  porter_stemmer = PorterStemmer()\n",
        "  words = word_tokenize(text)\n",
        "  stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "  stemmed_text = ' '.join(stemmed_words)\n",
        "  return stemmed_text\n",
        "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "stemmed_text = stem_text(text)\n",
        "print(stemmed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfhrRglPhzVT",
        "outputId": "05b3fd6a-efc0-4c7e-b1c9-cc7726bcbbbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk is a lead platform for build python program to work with human languag data .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=['running','jumping','thinking','happiness']\n",
        "porter_stemmer=PorterStemmer()\n",
        "snowball_stemmer=SnowballStemmer('english')\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "print(\"porter stemmer:\")\n",
        "for word in words:\n",
        "  print(f\"{word}->{porter_stemmer.stem(word)}\")\n",
        "print(\"\\nSnowball Stemmer:\")\n",
        "for word in words:\n",
        "  print(f\"{word}->{snowball_stemmer.stem(word)}\")\n",
        "print(\"\\n Word net lemmatizer:\")\n",
        "for word in words:\n",
        "  print(f\"{word}->{lemmatizer.lemmatize(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBhhz5UejtLp",
        "outputId": "5b8a5316-5129-4bbd-d79e-7d820b547816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "porter stemmer:\n",
            "running->run\n",
            "jumping->jump\n",
            "thinking->think\n",
            "happiness->happi\n",
            "\n",
            "Snowball Stemmer:\n",
            "running->run\n",
            "jumping->jump\n",
            "thinking->think\n",
            "happiness->happi\n",
            "\n",
            " Word net lemmatiaer:\n",
            "running->running\n",
            "jumping->jumping\n",
            "thinking->thinking\n",
            "happiness->happiness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "text=\"A Quick brown fox jumps over a lazy dog\"\n",
        "words = word_tokenize(text)\n",
        "pos_tags=pos_tag(words)\n",
        "print(\"pos Tags:\")\n",
        "for word,tag in pos_tags:\n",
        "  print(f\"{word}:{tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w6dnAUOlLha",
        "outputId": "e9aafac4-e914-4778-91d7-49e5d42a86a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos Tags:\n",
            "A:DT\n",
            "Quick:NNP\n",
            "brown:NN\n",
            "fox:NN\n",
            "jumps:VBZ\n",
            "over:IN\n",
            "a:DT\n",
            "lazy:JJ\n",
            "dog:NN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "def sentiment_analyser(text):\n",
        "  sia=SentimentIntensityAnalyzer()\n",
        "  Sentiment_scores=sia.polarity_scores(text)\n",
        "  if Sentiment_scores['compound']>=0.05:\n",
        "    Sentiment='positive'\n",
        "  elif Sentiment_scores['compound']<=-0.05:\n",
        "    Sentiment='negitive'\n",
        "  else:\n",
        "    Sentiment='natural'\n",
        "  return Sentiment_scores,Sentiment\n",
        "text=\"A quick brown fox jumped over a lazy dog\"\n",
        "Sentiment_scores,Sentiment=sentiment_analyser(text)\n",
        "print(f\"Sentiment: {Sentiment}\")\n",
        "print(f\"Sentiment Scores: {Sentiment_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lODtr3hKzRnQ",
        "outputId": "d00646ba-4d57-4874-e693-e19c0bc29af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: negitive\n",
            "Sentiment Scores: {'neg': 0.294, 'neu': 0.706, 'pos': 0.0, 'compound': -0.3612}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=spacy.load('en_core_web_sm')\n",
        "def named_entity_tokenize(text):\n",
        "  doc=nlp(text)\n",
        "  entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "  return entities\n",
        "text=\"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "entities=named_entity_tokenize(text)\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0h6RbrGujdK",
        "outputId": "6747a0ef-ac2e-488f-c214-cd96f8218d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import names\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('names')\n",
        "nltk.download('wordnet')\n",
        "dataset = [\n",
        "    ('This is a spam message.', 'spam'),\n",
        "    ('This is a legitimate message.', 'legitimate'),\n",
        "    ('Buy cheap viagra online.', 'spam'),\n",
        "    ('Hello, how are you?', 'legitimate'),\n",
        "\n",
        "]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def preprocess_data(data):\n",
        "    tokens = word_tokenize(data[0])\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return {token: True for token in tokens}, data[1]\n",
        "preprocessed_data = [preprocess_data(data) for data in dataset]\n",
        "train_set, test_set = preprocessed_data[:int(0.8 * len(preprocessed_data))], preprocessed_data[int(0.8 * len(preprocessed_data)):]\n",
        "\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "accuracy = nltk.classify.accuracy(classifier, test_set)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "def filter_spam(message):\n",
        "    tokens = word_tokenize(message)\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    features = {token: True for token in tokens}\n",
        "    return classifier.classify(features)\n",
        "message = \"Buy cheap viagra online.\"\n",
        "print(f\"Message: {message}\")\n",
        "print(f\"Classification: {filter_spam(message)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNnR2-tpuyOb",
        "outputId": "a960bbf7-d4a1-4476-cc29-f4aa727bfba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.00\n",
            "Message: Buy cheap viagra online.\n",
            "Classification: spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "dataset = [\n",
        "    ('This is a fake news article.', 'fake'),\n",
        "    ('This is a real news article.', 'real'),\n",
        "    ('The sky is blue.', 'real'),\n",
        "    ('The grass is purple.', 'fake'),]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def preprocess_data(data):\n",
        "    tokens = word_tokenize(data[0])\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens), data[1]\n",
        "\n",
        "preprocessed_data = [preprocess_data(data) for data in dataset]\n",
        "train_text, test_text, train_labels, test_labels = train_test_split([data[0] for data in preprocessed_data], [data[1] for data in preprocessed_data], test_size=0.2, random_state=42)\n",
        "vectorizer = TfidfVectorizer()\n",
        "train_vectors = vectorizer.fit_transform(train_text)\n",
        "test_vectors = vectorizer.transform(test_text)\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(train_vectors, train_labels)\n",
        "predictions = classifier.predict(test_vectors)\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(classification_report(test_labels, predictions))\n",
        "def detect_fake_news(text):\n",
        "    text = preprocess_data((text, 'fake'))[0]\n",
        "    vector = vectorizer.transform([text])\n",
        "    prediction = classifier.predict(vector)\n",
        "    return prediction[0]\n",
        "text = \"This is a fake news article.\"\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"Prediction: {detect_fake_news(text)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrelvRWcw0v0",
        "outputId": "48474425-b19f-4651-e3b7-e4c97a641d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.00\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.00      0.00      0.00       0.0\n",
            "        real       0.00      0.00      0.00       1.0\n",
            "\n",
            "    accuracy                           0.00       1.0\n",
            "   macro avg       0.00      0.00      0.00       1.0\n",
            "weighted avg       0.00      0.00      0.00       1.0\n",
            "\n",
            "Text: This is a fake news article.\n",
            "Prediction: fake\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1621ddb"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires retrieving all cells from the current notebook. The `get_cells()` function is used for this purpose.\n",
        "\n"
      ]
    }
  ]
}